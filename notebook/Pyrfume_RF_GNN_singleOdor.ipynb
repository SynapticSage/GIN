{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "79d06459",
   "metadata": {},
   "source": [
    "devintegration\n",
    "\n",
    "---\n",
    "jupyter:\n",
    "  jupytext:\n",
    "    text_representation:\n",
    "      extension: .py\n",
    "      format_name: light\n",
    "      format_version: '1.5'\n",
    "      jupytext_version: 1.16.3\n",
    "  kernelspec:\n",
    "    display_name: GIN\n",
    "    language: python\n",
    "    name: in\n",
    "---"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "ec0ec55a",
   "metadata": {
    "id": "LGz0DB7-1f0c"
   },
   "source": [
    "Name: Ryan Young\n",
    "\n",
    "Date: 2024-05-15"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "52dbc674",
   "metadata": {
    "id": "yejrQ-g50vYF"
   },
   "source": [
    "# Molecular Scent Analysis\n",
    "Hi!\n",
    "\n",
    "Welcome to this exploratory analysis notebook where we aim to predict whether a molecule might smell like a flower.\n",
    "\n",
    "This notebook's goal is to demonstrate an approach to solving a problem related to molecular scent prediction, with a focus on exploratory data analysis, model evaluation, and visualization of results.\n",
    "\n",
    "Towards the end, we will also explore some problems related to message passing and graph neural networks."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "a352184c",
   "metadata": {
    "id": "dtaFLFKizYVN"
   },
   "source": [
    "# Dependencies\n",
    "The following cell imports necessary libraries and modules, including a private GitHub repo called `gin` where I developed the code for this analysis.\n",
    "\n",
    "Even with notebooks, I tend to modularize pieces into repos for:\n",
    "- Reproducibility\n",
    "- CI/CD\n",
    "- Testing"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "023126c8",
   "metadata": {
    "id": "030e98c5-f7f4-4811-98a0-fc97b9cc0ce3"
   },
   "outputs": [],
   "source": [
    "# Imports and Argparse\n",
    "import importlib\n",
    "import os\n",
    "import shutil\n",
    "import argparse\n",
    "import datetime\n",
    "import matplotlib.pyplot as plt\n",
    "\n",
    "import gin\n",
    "\n",
    "import numpy as np\n",
    "import torch\n",
    "import pandas as pd  # Added pandas import\n",
    "from sklearn import ensemble as sklearn_ensemble\n",
    "from sklearn.model_selection import train_test_split\n",
    "from imblearn.over_sampling import SMOTE\n",
    "\n",
    "# New MLflow imports and initialization\n",
    "import mlflow\n",
    "from gin.log.mlflow import describe_run, log_params, log_metrics, log_artifacts\n",
    "\n",
    "parser = argparse.ArgumentParser(\n",
    "    description='Predict the presence of a specific odor descriptor.')\n",
    "parser.add_argument('--archive', \n",
    "                    type=str, \n",
    "                    default='leffingwell',\n",
    "                    help='Name of the Pyrfume data archive to use.')\n",
    "parser.add_argument('--descriptor', \n",
    "                    type=str, \n",
    "                    default='floral',\n",
    "                    help='The odor descriptor to predict.')\n",
    "args = parser.parse_args(args=[])  # Added args=[] for notebook execution\n",
    "desc = args.descriptor\n",
    "args.script = \"Pyrfume_RF_GNN_singleOdor.py\"\n",
    "\n",
    "print(\" ------  ARGS -------- \")\n",
    "print(args)\n",
    "print(\" --------------------- \")\n",
    "\n",
    "# import seaborn as sns\n",
    "plt.rcParams['figure.dpi'] = 150\n",
    "\n",
    "# Save the figure\n",
    "figure_dir = os.path.join(os.path.dirname(gin.__file__), '..', 'figures', args.descriptor)\n",
    "print(\"Figure directory:\", figure_dir)\n",
    "os.makedirs(figure_dir, exist_ok=True)  # Create the directory if it doesn't exist\n",
    "figure_path = (lambda x=\"\": \n",
    "                os.path.join(figure_dir, f'{plt.gcf().get_suptitle() if not x else x}.png'))\n",
    "df_path = os.path.join(figure_dir, \"..\", \"df.csv\")  # WARNING: in the face of more analyses, may have to split this dataframe\n",
    "save_fig = lambda x=\"\": plt.savefig(figure_path(x))"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "1a5f54e8",
   "metadata": {
    "id": "setup_mlflow"
   },
   "source": [
    "## MLFLOW Setup\n",
    "This section sets up experiment tracking with MLflow."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "049c4298",
   "metadata": {
    "id": "setup_mlflow_code"
   },
   "outputs": [],
   "source": [
    "# Extract experiment name and run name using describe_run\n",
    "exp_name, run_name = describe_run(args.__dict__, name=\"Pyrfume_RF_GNN_singleOdor\")\n",
    "\n",
    "# Set the MLflow experiment\n",
    "mlflow.set_experiment(exp_name)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "a90921ba",
   "metadata": {
    "id": "d538e6fc"
   },
   "source": [
    "# Dataset\n",
    "\n",
    "Here we will use data managed by [the Pyrfume project](https://pyrfume.org/).\n",
    "\n",
    "The [SMILES strings](https://en.wikipedia.org/wiki/Simplified_molecular-input_line-entry_system) representing the molecular structures and their corresponding binary labels are provided."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "78289796",
   "metadata": {
    "id": "load_data"
   },
   "outputs": [],
   "source": [
    "# Load the data\n",
    "data_df = gin.data.pyrfume.get_join(args.archive, \n",
    "                                    types=[\"behavior\", \"molecules\", \"stimuli\"])\n",
    "data_df = pd.DataFrame(data_df.set_index('SMILES')[desc])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "71c440e1",
   "metadata": {
    "id": "data_head"
   },
   "outputs": [],
   "source": [
    "data_df.head()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "1eadaf30",
   "metadata": {
    "id": "af266b7c"
   },
   "source": [
    "Now that we have the data loaded, what should we learn about this dataset?"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "b9661ef6",
   "metadata": {
    "id": "check_nulls"
   },
   "outputs": [],
   "source": [
    "data_df[desc].isnull().sum()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "41c18ae5",
   "metadata": {
    "id": "2fd33ec1"
   },
   "source": [
    "No missing values - reassuring!\n",
    "\n",
    "Let's see the distribution of the labels in the dataset."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "8a6439dd",
   "metadata": {
    "id": "plot_distribution"
   },
   "outputs": [],
   "source": [
    "gin.explore.pyrfume.plot_desc_distribution(data_df, kind='pie', descriptor=desc)\n",
    "save_fig(f'{desc}_distribution')\n",
    "mlflow.log_artifact(f'{figure_dir}/{desc}_distribution.png')\n",
    "\n",
    "# Log metrics for class distribution\n",
    "log_metrics({\"class_distribution\": data_df[desc].value_counts().to_dict()})"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "4aa03f46",
   "metadata": {
    "id": "d9d9222f"
   },
   "source": [
    "👆 The large majority of the dataset is non-floral ❌💐. We should consider **class imbalance** downstream."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "1d1f58c5",
   "metadata": {
    "id": "visualize_molecules"
   },
   "outputs": [],
   "source": [
    "# Let's visualize some of the molecular structures in the dataset and see if we can spot any patterns.\n",
    "gin.explore.pyrfume.plot_molecular_structures_w_label(data_df, num_samples=20, descriptor=desc)\n",
    "save_fig(f'{desc}_molecular_structures_1')\n",
    "mlflow.log_artifact(f'{figure_dir}/{desc}_molecular_structures_1.png')"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "508b9e7a",
   "metadata": {
    "id": "2059e7bf"
   },
   "source": [
    "And let's examine a few more samples."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "3a173203",
   "metadata": {
    "id": "visualize_molecules_2",
    "lines_to_next_cell": 1
   },
   "outputs": [],
   "source": [
    "gin.explore.pyrfume.plot_molecular_structures_w_label(data_df, num_samples=20, descriptor=desc)\n",
    "save_fig(f'{desc}_molecular_structures_2')\n",
    "mlflow.log_artifact(f'{figure_dir}/{desc}_molecular_structures_2.png')"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "fa04caf9",
   "metadata": {
    "id": "3b8a8363"
   },
   "source": [
    "## Hypotheses\n",
    "\n",
    "Some things of note just from the visualization (hypotheses / possibilities / wild guesses):\n",
    "\n",
    "- The 🪻 floral molecules nearly all have oxygen with free electron pairs. Doubled-bond oxygen alone seems less often associated with floral molecules.\n",
    "- Nitrogen-containing molecules are rarely floral - though, devil's advocate, I also see fewer nitrogen-containing molecules to form an opinion.\n",
    "\n",
    "# Molecule Featurization\n",
    "\n",
    "In the next step, we will try to \"digitize\" each molecule by creating a 1D numpy array based on its molecular structure."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "c469d783",
   "metadata": {
    "id": "featurize_smiles"
   },
   "outputs": [],
   "source": [
    "def featurize_smiles(smiles_str: str,\n",
    "                     method: str = 'combined') -> np.ndarray:\n",
    "    \"\"\"Convert a molecule SMILES into a 1D feature vector.\"\"\"\n",
    "    if method == 'morgan':\n",
    "        fingerprint = gin.features.get_morgan_fingerprint(smiles_str)\n",
    "    elif method == 'maccs':\n",
    "        fingerprint = gin.features.get_maccs_keys_fingerprint(smiles_str)\n",
    "    elif method == 'combined':\n",
    "        fingerprint = gin.features.get_combined_fingerprint(smiles_str)\n",
    "    else:\n",
    "        raise ValueError(f\"Invalid method: {method}\")\n",
    "    return fingerprint\n",
    "\n",
    "# Test the function\n",
    "featurize_smiles('CC(C)CC(C)(O)C1CCCS1')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "8372b992",
   "metadata": {
    "id": "construct_features"
   },
   "outputs": [],
   "source": [
    "# Construct the features `x` and labels `y` for the model\n",
    "x = np.array([featurize_smiles(v) for v in data_df.index])\n",
    "from sklearn.preprocessing import OrdinalEncoder\n",
    "label_encoder = OrdinalEncoder()\n",
    "x = label_encoder.fit_transform(x)\n",
    "y = data_df[desc].values\n",
    "gin.explore.pyrfume.plot_feature_heatmap(x)\n",
    "save_fig(f'{desc}_feature_heatmap')\n",
    "mlflow.log_artifact(f'{figure_dir}/{desc}_feature_heatmap.png')"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "13926f71",
   "metadata": {
    "id": "604a9a65"
   },
   "source": [
    "Having noticed the above, we should maybe be thinking about the following:\n",
    "\n",
    "- Feature scaling - less necessary for tree-based models\n",
    "- High class cardinality - hopefully not an issue\n",
    "- Feature imbalance - this is a possible issue, but we can address this\n",
    "  - SMOTE is an option for increasing the minority class\n",
    "\n",
    "## Splitting the data, cross-validation\n",
    "We have to split the data into training and testing sets."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "a69885d1",
   "metadata": {
    "id": "split_data"
   },
   "outputs": [],
   "source": [
    "X_train, X_test, y_train, y_test = train_test_split(x, y, test_size=0.2, random_state=42)\n",
    "\n",
    "smote = SMOTE()\n",
    "# Resampling the training data to address class imbalance\n",
    "X_train_res, y_train_res = smote.fit_resample(X_train, y_train)\n",
    "X_test_res, y_test_res = smote.fit_resample(X_test, y_test)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "3d80a5b6",
   "metadata": {
    "id": "6b56123a"
   },
   "source": [
    "## Train and evaluate a Random Forest (RF) model\n",
    "\n",
    "We will use the RF implementation from [scikit-learn](https://scikit-learn.org/stable/modules/generated/sklearn.ensemble.RandomForestClassifier.html)."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "0ea8a6cf",
   "metadata": {
    "id": "train_rf"
   },
   "outputs": [],
   "source": [
    "# Start a parent run for the RandomForest models\n",
    "mlflow.start_run(run_name=\"RandomForest_parent\")\n",
    "\n",
    "# What hyper-parameters should we use?\n",
    "best_params = {'bootstrap': False, \n",
    "               'max_depth': None, \n",
    "               'max_features': 'log2', \n",
    "               'min_samples_leaf': 1, \n",
    "               'min_samples_split': 5, \n",
    "               'n_estimators': 300}  # WARNING: tuned on Floral molecules -- may not apply to others\n",
    "\n",
    "log_params({'rf_' + key: value for key, value in best_params.items()})\n",
    "\n",
    "model = sklearn_ensemble.RandomForestClassifier(**best_params)\n",
    "model_res = sklearn_ensemble.RandomForestClassifier(**best_params)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "70af316f",
   "metadata": {
    "id": "rf_model_1"
   },
   "outputs": [],
   "source": [
    "# Start a child run for the first RandomForest model\n",
    "mlflow.start_run(run_name=\"RandomForest_model_1\", nested=True)\n",
    "\n",
    "# Fit and predict\n",
    "rf_y_pred = model.fit(X_train, y_train).predict(X_test)\n",
    "\n",
    "# Log metrics for RandomForest model_1\n",
    "metrics_rf = gin.validate.get_metrics(y_test, rf_y_pred)\n",
    "log_metrics(metrics_rf)\n",
    "\n",
    "# Log confusion matrix\n",
    "gin.validate.plot_confusion_matrix(y_test, rf_y_pred, suptitle=\"Random Forest\")\n",
    "save_fig(f'{desc}_confusion_matrix_rf')\n",
    "mlflow.log_artifact(f'{figure_dir}/{desc}_confusion_matrix_rf.png')\n",
    "\n",
    "# End child run\n",
    "mlflow.end_run()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "cf49364f",
   "metadata": {
    "id": "rf_model_resampled"
   },
   "outputs": [],
   "source": [
    "# Start a child run for the RandomForest model with resampled data\n",
    "mlflow.start_run(run_name=\"RandomForest_model_resampled\", nested=True)\n",
    "\n",
    "# Fit and predict\n",
    "rf_y_pred_res = model_res.fit(X_train_res, y_train_res).predict(X_test_res)\n",
    "rf_y_pred_res2uns = model_res.predict(X_test)\n",
    "\n",
    "# Log metrics for RandomForest model_resampled\n",
    "metrics_rf_resampled = gin.validate.get_metrics(y_test, rf_y_pred_res2uns)\n",
    "log_metrics(metrics_rf_resampled)\n",
    "\n",
    "# Log confusion matrix\n",
    "gin.validate.plot_confusion_matrix(y_test, rf_y_pred_res2uns, suptitle=\"Random Forest - Resampled\")\n",
    "save_fig(f'{desc}_confusion_matrix_rf_resampled')\n",
    "mlflow.log_artifact(f'{figure_dir}/{desc}_confusion_matrix_rf_resampled.png')\n",
    "\n",
    "# End child run\n",
    "mlflow.end_run()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "8a92059a",
   "metadata": {
    "id": "rf_thresholds"
   },
   "outputs": [],
   "source": [
    "# Evaluate thresholds for Random Forest models\n",
    "thresholds = np.arange(0, 1, 0.01)\n",
    "results_df = gin.validate.evaluate_thresholds(model, X_test, y_test, thresholds)\n",
    "results_df_res = gin.validate.evaluate_thresholds(model_res, X_test, y_test, thresholds)\n",
    "\n",
    "# Plot threshold results\n",
    "gin.validate.plot_threshold_results(results_df, model_name='Random Forest', suptitle='Random Forest')\n",
    "save_fig(f'{desc}_threshold_results_rf')\n",
    "mlflow.log_artifact(f'{figure_dir}/{desc}_threshold_results_rf.png')\n",
    "\n",
    "gin.validate.plot_threshold_results(results_df_res, model_name='Random Forest - Resampled', suptitle='Random Forest - Resampled')\n",
    "save_fig(f'{desc}_threshold_results_rf_resampled')\n",
    "mlflow.log_artifact(f'{figure_dir}/{desc}_threshold_results_rf_resampled.png')\n",
    "\n",
    "# End parent run for RandomForest\n",
    "mlflow.end_run()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "0c430904",
   "metadata": {
    "id": "f18bca3d"
   },
   "source": [
    "And out of curiosity, let's also try an ensemble - even though for production-level models, this is likely overkill. A tiny performance boost often isn't worth the time and complexity."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "6a9e4d48",
   "metadata": {
    "id": "ensemble"
   },
   "outputs": [],
   "source": [
    "# Start a parent run for Ensemble models\n",
    "mlflow.start_run(run_name=\"Ensemble_parent\")\n",
    "\n",
    "# Define classifiers\n",
    "from sklearn.linear_model import LogisticRegression\n",
    "clf1 = LogisticRegression(max_iter=1000)\n",
    "clf2 = sklearn_ensemble.RandomForestClassifier(**best_params)\n",
    "clf3 = sklearn_ensemble.GradientBoostingClassifier()\n",
    "\n",
    "# VotingClassifier with hard voting\n",
    "model_vote = sklearn_ensemble.VotingClassifier(estimators=[('lr', clf1), ('rf', clf2), ('gb', clf3)], voting='hard')\n",
    "model_vote_res = sklearn_ensemble.VotingClassifier(estimators=[('lr', clf1), ('rf', clf2), ('gb', clf3)], voting='hard')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "03523093",
   "metadata": {
    "id": "ensemble_model_1"
   },
   "outputs": [],
   "source": [
    "# Start a child run for the ensemble model\n",
    "mlflow.start_run(run_name=\"Ensemble_model_1\", nested=True)\n",
    "\n",
    "# Fit and predict\n",
    "model_vote.fit(X_train, y_train)\n",
    "eclf_y_pred = model_vote.predict(X_test)\n",
    "\n",
    "# Log metrics\n",
    "metrics_ensemble = gin.validate.get_metrics(y_test, eclf_y_pred)\n",
    "log_metrics(metrics_ensemble)\n",
    "\n",
    "# Log confusion matrix\n",
    "gin.validate.plot_confusion_matrix(y_test, eclf_y_pred, suptitle=\"Ensemble\")\n",
    "save_fig(f'{desc}_confusion_matrix_ensemble')\n",
    "mlflow.log_artifact(f'{figure_dir}/{desc}_confusion_matrix_ensemble.png')\n",
    "\n",
    "# End child run\n",
    "mlflow.end_run()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "d524f01e",
   "metadata": {
    "id": "ensemble_model_resampled"
   },
   "outputs": [],
   "source": [
    "# Start a child run for the ensemble model with resampled data\n",
    "mlflow.start_run(run_name=\"Ensemble_model_resampled\", nested=True)\n",
    "\n",
    "# Fit and predict\n",
    "model_vote_res.fit(X_train_res, y_train_res)\n",
    "eclf_y_pred_res2uns = model_vote_res.predict(X_test)\n",
    "\n",
    "# Log metrics\n",
    "metrics_ensemble_resampled = gin.validate.get_metrics(y_test, eclf_y_pred_res2uns)\n",
    "log_metrics(metrics_ensemble_resampled)\n",
    "\n",
    "# Log confusion matrix\n",
    "gin.validate.plot_confusion_matrix(y_test, eclf_y_pred_res2uns, suptitle=\"Ensemble - Resampled\")\n",
    "save_fig(f'{desc}_confusion_matrix_ensemble_resampled')\n",
    "mlflow.log_artifact(f'{figure_dir}/{desc}_confusion_matrix_ensemble_resampled.png')\n",
    "\n",
    "# End child run\n",
    "mlflow.end_run()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "b961d134",
   "metadata": {
    "id": "ensemble_thresholds"
   },
   "outputs": [],
   "source": [
    "# Evaluate thresholds for Ensemble models\n",
    "results_df_ensemble = gin.validate.evaluate_thresholds(model_vote, X_test, y_test, thresholds)\n",
    "results_df_ensemble_res = gin.validate.evaluate_thresholds(model_vote_res, X_test, y_test, thresholds)\n",
    "\n",
    "# Plot threshold results\n",
    "gin.validate.plot_threshold_results(results_df_ensemble, model_name='Ensemble', suptitle='Ensemble')\n",
    "save_fig(f'{desc}_threshold_results_ensemble')\n",
    "mlflow.log_artifact(f'{figure_dir}/{desc}_threshold_results_ensemble.png')\n",
    "\n",
    "gin.validate.plot_threshold_results(results_df_ensemble_res, model_name='Ensemble - Resampled', suptitle='Ensemble - Resampled')\n",
    "save_fig(f'{desc}_threshold_results_ensemble_resampled')\n",
    "mlflow.log_artifact(f'{figure_dir}/{desc}_threshold_results_ensemble_resampled.png')\n",
    "\n",
    "# End parent run for Ensemble\n",
    "mlflow.end_run()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "58aadf49",
   "metadata": {
    "id": "b043bdab"
   },
   "source": [
    "# Multi-layer Perceptron (MLP)\n",
    "Let's train a simple neural network.\n",
    "\n",
    "Now that we have tried modeling with an RF, let's try modeling with a simple neural network: the [Multilayer Perceptron](https://en.wikipedia.org/wiki/Multilayer_perceptron)."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "09808a86",
   "metadata": {
    "id": "train_mlp"
   },
   "outputs": [],
   "source": [
    "# Start a parent run for MLP models\n",
    "mlflow.start_run(run_name=\"MLP_parent\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "4f7afde8",
   "metadata": {
    "id": "mlp_model_1"
   },
   "outputs": [],
   "source": [
    "# Start a child run for MLP model\n",
    "mlflow.start_run(run_name=\"MLP_model_1\", nested=True)\n",
    "\n",
    "from gin.model import MLP\n",
    "model_mlp = MLP(input_dim=X_train.shape[1])\n",
    "model_mlp.fit(X_train, y_train)\n",
    "\n",
    "# Predict\n",
    "mlp_y_pred = model_mlp.predict(X_test)\n",
    "\n",
    "# Log metrics\n",
    "metrics_mlp = gin.validate.get_metrics(y_test, mlp_y_pred > 0.5)\n",
    "log_metrics(metrics_mlp)\n",
    "\n",
    "# Log confusion matrix\n",
    "gin.validate.plot_confusion_matrix(y_test, mlp_y_pred > 0.5, suptitle=\"MLP\")\n",
    "save_fig(f'{desc}_confusion_matrix_mlp')\n",
    "mlflow.log_artifact(f'{figure_dir}/{desc}_confusion_matrix_mlp.png')\n",
    "\n",
    "# End child run\n",
    "mlflow.end_run()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "57376f52",
   "metadata": {
    "id": "mlp_model_resampled"
   },
   "outputs": [],
   "source": [
    "# Start a child run for MLP model with resampled data\n",
    "mlflow.start_run(run_name=\"MLP_model_resampled\", nested=True)\n",
    "\n",
    "model_mlp_res = MLP(input_dim=X_train_res.shape[1])\n",
    "model_mlp_res.fit(X_train_res, y_train_res)\n",
    "\n",
    "# Predict\n",
    "mlp_y_pred_res = model_mlp_res.predict(X_test)\n",
    "\n",
    "# Log metrics\n",
    "metrics_mlp_resampled = gin.validate.get_metrics(y_test, mlp_y_pred_res > 0.5)\n",
    "log_metrics(metrics_mlp_resampled)\n",
    "\n",
    "# Log confusion matrix\n",
    "gin.validate.plot_confusion_matrix(y_test, mlp_y_pred_res > 0.5, suptitle=\"MLP - Resampled\")\n",
    "save_fig(f'{desc}_confusion_matrix_mlp_resampled')\n",
    "mlflow.log_artifact(f'{figure_dir}/{desc}_confusion_matrix_mlp_resampled.png')\n",
    "\n",
    "# End child run\n",
    "mlflow.end_run()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "0f48b73d",
   "metadata": {
    "id": "mlp_thresholds"
   },
   "outputs": [],
   "source": [
    "# Evaluate thresholds for MLP models\n",
    "results_df_mlp = gin.validate.evaluate_thresholds(model_mlp, X_test, y_test, thresholds, y_proba=mlp_y_pred)\n",
    "results_df_mlp_res = gin.validate.evaluate_thresholds(model_mlp_res, X_test, y_test, thresholds, y_proba=mlp_y_pred_res)\n",
    "\n",
    "# Plot threshold results\n",
    "gin.validate.plot_threshold_results(results_df_mlp, model_name='MLP', suptitle='MLP')\n",
    "save_fig(f'{desc}_threshold_results_mlp')\n",
    "mlflow.log_artifact(f'{figure_dir}/{desc}_threshold_results_mlp.png')\n",
    "\n",
    "gin.validate.plot_threshold_results(results_df_mlp_res, model_name='MLP - Resampled', suptitle='MLP - Resampled')\n",
    "save_fig(f'{desc}_threshold_results_mlp_resampled')\n",
    "mlflow.log_artifact(f'{figure_dir}/{desc}_threshold_results_mlp_resampled.png')\n",
    "\n",
    "# End parent run for MLP\n",
    "mlflow.end_run()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "5de284ba",
   "metadata": {
    "id": "5b528b92"
   },
   "source": [
    "## Conclusions\n",
    "\n",
    "The `MLP` achieves a very similar performance to the simpler methods above.\n",
    "\n",
    "Notably, the resampled model for the `MLP` does **not** perform any better, unlike the `RandomForest` above. In practice, we could try other methods of rebalancing and data augmentation techniques given sparse samples."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "4e893989",
   "metadata": {
    "id": "bcbbe320-e334-4f9f-ab4e-bc9b72083a85"
   },
   "source": [
    "# Graph Neural Network (GNN), Bonus - Naive (👶)\n",
    "\n",
    "For fun, let's dovetail this section with a very naive message-passing GNN approach 🤖\n",
    "\n",
    "*NOTES OF INTEREST* 📝\n",
    "- We are doing this without resampling/data-augmentation -- so we _may not approach_ performance above.\n",
    "- Instead, using a simpler class-imbalance reweighting function in the cross-entropy objective.\n",
    "- We may not have enough samples to utilize the capacity of a bigger model."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "808da808",
   "metadata": {
    "id": "gnn_setup"
   },
   "outputs": [],
   "source": [
    "# Start a parent run for GNN models\n",
    "mlflow.start_run(run_name=\"GNN_parent\")\n",
    "\n",
    "# Convert the SMILES strings to graph data and split into train/test sets\n",
    "from sklearn.model_selection import train_test_split\n",
    "import torch\n",
    "from torch_geometric.data import DataLoader\n",
    "from gin.extra.features import smiles_to_graph\n",
    "from gin.extra.gnn import train_gnn_model\n",
    "\n",
    "# Convert the SMILES strings to graph data\n",
    "data_list = []\n",
    "for smile_string, floral in zip(data_df.index, data_df[desc]):\n",
    "    data = smiles_to_graph(smile_string)\n",
    "    if data is not None:\n",
    "        data.y = torch.tensor([floral], dtype=torch.float)  # Assign target value\n",
    "        data_list.append(data)\n",
    "data_list = gin.extra.features.normalize_data_list(data_list)  # Normalize features\n",
    "\n",
    "if len(data_list) == 0:\n",
    "    raise ValueError(\"No valid graph data could be generated from the provided SMILES strings.\")\n",
    "\n",
    "# Split the data into training and testing sets\n",
    "train_data, test_data = train_test_split(data_list, test_size=0.2, random_state=42)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "71b3fe70",
   "metadata": {
    "id": "gnn_model_1"
   },
   "outputs": [],
   "source": [
    "# Start a child run for the GNN model\n",
    "mlflow.start_run(run_name=\"GNN_model_1\", nested=True)\n",
    "\n",
    "# Train the GNN model on the training data\n",
    "model_gnn = train_gnn_model(train_data, num_epochs=250)\n",
    "\n",
    "# End child run\n",
    "mlflow.end_run()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "3b58821b",
   "metadata": {
    "id": "gnn_inference"
   },
   "outputs": [],
   "source": [
    "# Run inference\n",
    "model_gnn.eval()\n",
    "test_loader = DataLoader(test_data, batch_size=32, shuffle=False)\n",
    "\n",
    "all_preds = []\n",
    "all_labels = []\n",
    "\n",
    "with torch.no_grad():\n",
    "    for batch in test_loader:\n",
    "        preds = model_gnn(batch.x, batch.edge_index, batch.edge_attr, batch.batch)\n",
    "        all_preds.extend(preds.numpy().flatten())\n",
    "        all_labels.extend(batch.y.numpy().flatten())\n",
    "\n",
    "# Plot distributions\n",
    "import matplotlib.pyplot as plt\n",
    "\n",
    "plt.hist(all_preds, bins=20, alpha=0.75, label='Predictions')\n",
    "plt.hist(all_labels, bins=20, alpha=0.75, label='True Labels')\n",
    "plt.legend()\n",
    "plt.title('Distribution of Predictions and True Labels')\n",
    "save_fig(f'{desc}_gnn_predictions_distribution')\n",
    "mlflow.log_artifact(f'{figure_dir}/{desc}_gnn_predictions_distribution.png')\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "30a94c96",
   "metadata": {
    "id": "gnn_evaluation"
   },
   "outputs": [],
   "source": [
    "# Evaluate performance\n",
    "all_preds = np.array(all_preds)\n",
    "all_labels = np.array(all_labels)\n",
    "metrics_gnn = gin.validate.get_metrics(all_labels, all_preds > 0.5)\n",
    "log_metrics(metrics_gnn)\n",
    "\n",
    "# Log confusion matrix\n",
    "gin.validate.plot_confusion_matrix(all_labels, all_preds > 0.5, suptitle='GNN Model')\n",
    "save_fig(f'{desc}_confusion_matrix_gnn')\n",
    "mlflow.log_artifact(f'{figure_dir}/{desc}_confusion_matrix_gnn.png')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "4f848f1b",
   "metadata": {
    "id": "gnn_thresholds"
   },
   "outputs": [],
   "source": [
    "# Evaluate thresholds for GNN model\n",
    "from gin.extra.validate import evaluate_thresholds_gnn\n",
    "thresholds = np.arange(0.0, 1.0, 0.01)\n",
    "results_gnn = evaluate_thresholds_gnn(model_gnn, test_data, thresholds)\n",
    "\n",
    "# Plot threshold results\n",
    "gin.validate.plot_threshold_results(results_gnn, model_name=\"GNN\")\n",
    "save_fig(f'{desc}_threshold_results_gnn')\n",
    "mlflow.log_artifact(f'{figure_dir}/{desc}_threshold_results_gnn.png')\n",
    "\n",
    "# End parent run for GNN\n",
    "mlflow.end_run()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "73090996",
   "metadata": {
    "id": "5b528b92"
   },
   "source": [
    "## Conclusions\n",
    "\n",
    "The `GNN` model, while an interesting exercise, does not perform as well as the simpler models. This is typical for neural networks with smaller datasets.\n",
    "\n",
    "**Better yet** -- pull a model from HuggingFace 🤗 that has been pre-trained on other molecules to leverage the knowledge seen in other data.\n",
    "\n",
    "# The End"
   ]
  }
 ],
 "metadata": {
  "jupytext": {
   "cell_metadata_filter": "id,-all",
   "main_language": "python",
   "notebook_metadata_filter": "-all"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
